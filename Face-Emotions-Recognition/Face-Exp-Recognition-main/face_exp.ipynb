{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "face-exp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPvVwqvl+s14eg14O0DfCx8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aditi-saxena-1206/Face-Exp-Recognition/blob/main/face_exp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHL7wNgRFDR7",
        "outputId": "b9e4e739-7bbf-4a6f-efea-f9ad385927cc"
      },
      "source": [
        "#mounting drive on colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNi3Ahij2H4l",
        "outputId": "c8b601bd-e3ec-4c76-a3fd-39704432c9c2"
      },
      "source": [
        "# model 1\n",
        "#importing required libraries\n",
        "import sys\n",
        "import os\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "\n",
        "#deep learning libraries\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Activation, Flatten, BatchNormalization\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, TensorBoard, ModelCheckpoint\n",
        "\n",
        "#load dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Expression recognition/needed/fer2013.csv\")\n",
        "\n",
        "#data visualization\n",
        "print(df.info())\n",
        "print(df.columns)\n",
        "print(df.head())\n",
        "print(df.Usage.unique())\n",
        "\n",
        "#data splitting\n",
        "X_train = []\n",
        "Y_train = []\n",
        "X_val = []\n",
        "Y_val = []\n",
        "X_test = []\n",
        "Y_test = []\n",
        "\n",
        "\n",
        "for index,row in df.iterrows():\n",
        "    pix = row['pixels'].split(' ')\n",
        "    if row['Usage'] == 'Training':\n",
        "        X_train.append(np.array(pix,'float32'))\n",
        "        Y_train.append(row['emotion'])\n",
        "    elif row['Usage'] == \"PublicTest\":\n",
        "        X_val.append(np.array(pix,'float32'))\n",
        "        Y_val.append(row['emotion'])\n",
        "    elif row['Usage'] == \"PrivateTest\":\n",
        "        X_test.append(np.array(pix,'float32'))\n",
        "        Y_test.append(row['emotion'])\n",
        "\n",
        "#convert to numpy array\n",
        "X_train = np.array(X_train,'float32')\n",
        "Y_train = np.array(Y_train,'float32')\n",
        "X_val = np.array(X_val,'float32')\n",
        "Y_val = np.array(Y_val,'float32')\n",
        "X_test = np.array(X_test,'float32')\n",
        "Y_test = np.array(Y_test,'float32')\n",
        "\n",
        "print(X_train.shape)\n",
        "\n",
        "#convert to one-hot encoding\n",
        "Y_train = np_utils.to_categorical(Y_train, num_classes=7)\n",
        "Y_test = np_utils.to_categorical(Y_test, num_classes=7)\n",
        "Y_val = np_utils.to_categorical(Y_val, num_classes=7)\n",
        "\n",
        "#reshape to 2D\n",
        "X_train = X_train.reshape(X_train.shape[0], 48, 48, 1)\n",
        "X_val = X_val.reshape(X_val.shape[0],48,48,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 48, 48, 1)\n",
        "\n",
        "print(X_train.shape)\n",
        "\n",
        "#CNN model\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(64, kernel_size = (3,3), activation = 'relu', input_shape = (48,48,1), kernel_regularizer=l2(0.01)))\n",
        "model.add(Conv2D(64,kernel_size = (3,3), activation = 'relu', padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(128, kernel_size = (3,3), activation = 'relu', padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128, kernel_size = (3,3), activation = 'relu', padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128, kernel_size = (3,3), activation = 'relu', padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(256, kernel_size = (3,3), activation = 'relu',padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(256, kernel_size = (3,3), activation = 'relu',padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(256, kernel_size = (3,3), activation = 'relu',padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(512, kernel_size = (3,3), activation = 'relu',padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(512, kernel_size = (3,3), activation = 'relu',padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(512, kernel_size = (3,3), activation = 'relu',padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512,activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(256,activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(128,activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(64,activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(7,activation='softmax'))\n",
        "\n",
        "#define callbacks\n",
        "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3)\n",
        "early_stopper = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=6, mode='auto')\n",
        "checkpointer = ModelCheckpoint('/content/drive/MyDrive/Expression recognition/needed/weights.hd5', monitor='val_loss', verbose=1, save_best_only=True)\n",
        "\n",
        "#compile model\n",
        "model.compile(loss = categorical_crossentropy, optimizer=Adam(lr=0.001), metrics=['accuracy'] )\n",
        "\n",
        "#training model\n",
        "print(\"Training model\")\n",
        "model.fit(X_train, Y_train, batch_size=64, epochs = 50, verbose=1, validation_data=(X_val,Y_val), shuffle = True, callbacks=[lr_reducer, checkpointer, early_stopper] )\n",
        "\n",
        "#save the model and weights\n",
        "model_json = model.to_json()\n",
        "with open(\"model.json\",'w') as json_file:\n",
        "    json_file.write(model_json)\n",
        "print(\"Model saved\")\n",
        "model.save_weights(\"weights.h5\")\n",
        "print(\"Weights saved\")\n",
        "\n",
        "#prediction\n",
        "Y_pred = model.predict(X_test)\n",
        "Y_pred = np.argmax(Y_pred, axis=-1)\n",
        "#Y_test = Y_test.to_numpy()\n",
        "Y_test.flatten()\n",
        "Y_test.shape\n",
        "from sklearn.metrics import accuracy_score\n",
        "Y_pred = np_utils.to_categorical(Y_pred, num_classes=7)\n",
        "print(accuracy_score(Y_test, Y_pred))\n",
        "\n",
        "# accuracy = 0.2449150181108944\n",
        "#loss: 1.8121 - accuracy: 0.2522 - val_loss: 1.8097 - val_accuracy: 0.2494"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 35887 entries, 0 to 35886\n",
            "Data columns (total 3 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   emotion  35887 non-null  int64 \n",
            " 1   pixels   35887 non-null  object\n",
            " 2   Usage    35887 non-null  object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 841.2+ KB\n",
            "None\n",
            "Index(['emotion', 'pixels', 'Usage'], dtype='object')\n",
            "   emotion                                             pixels     Usage\n",
            "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
            "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
            "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
            "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
            "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training\n",
            "['Training' 'PublicTest' 'PrivateTest']\n",
            "(28709, 2304)\n",
            "(28709, 48, 48, 1)\n",
            "Training model\n",
            "Epoch 1/50\n",
            "449/449 [==============================] - 17s 35ms/step - loss: 2.4910 - accuracy: 0.1862 - val_loss: 1.8829 - val_accuracy: 0.2494\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.88289, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 2/50\n",
            "449/449 [==============================] - 15s 34ms/step - loss: 1.8670 - accuracy: 0.2338 - val_loss: 1.8429 - val_accuracy: 0.2494\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.88289 to 1.84291, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 3/50\n",
            "449/449 [==============================] - 15s 34ms/step - loss: 1.8514 - accuracy: 0.2419 - val_loss: 1.8292 - val_accuracy: 0.2494\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.84291 to 1.82919, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 4/50\n",
            "449/449 [==============================] - 15s 34ms/step - loss: 1.8305 - accuracy: 0.2507 - val_loss: 1.8224 - val_accuracy: 0.2494\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.82919 to 1.82241, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 5/50\n",
            "449/449 [==============================] - 15s 34ms/step - loss: 1.8257 - accuracy: 0.2502 - val_loss: 1.8181 - val_accuracy: 0.2494\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.82241 to 1.81814, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 6/50\n",
            "449/449 [==============================] - 15s 34ms/step - loss: 1.8172 - accuracy: 0.2492 - val_loss: 1.8135 - val_accuracy: 0.2494\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.81814 to 1.81354, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 7/50\n",
            "449/449 [==============================] - 15s 34ms/step - loss: 1.8121 - accuracy: 0.2522 - val_loss: 1.8097 - val_accuracy: 0.2494\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.81354 to 1.80970, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Model saved\n",
            "Weights saved\n",
            "0.2449150181108944\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmMuvKLI75F6"
      },
      "source": [
        "#model 2 - no validation data, data augmentation \n",
        "# importing required libraries\n",
        "import sys\n",
        "import os\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Activation, Flatten, BatchNormalization\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, TensorBoard, ModelCheckpoint\n",
        "\n",
        "#load dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Expression recognition/needed/fer2013.csv\")\n",
        "\n",
        "#data visualization\n",
        "print(df.info())\n",
        "print(df.columns)\n",
        "print(df.head())\n",
        "print(df.Usage.unique())\n",
        "\n",
        "#data splitting\n",
        "X_train = []\n",
        "Y_train = []\n",
        "X_test = []\n",
        "Y_test = []\n",
        "\n",
        "for index,row in df.iterrows():\n",
        "    pix = row['pixels'].split(' ')\n",
        "    if row['Usage'] == 'PrivateTest':\n",
        "        X_test.append(np.array(pix,'float32'))\n",
        "        Y_test.append(row['emotion'])\n",
        "    else:\n",
        "        X_train.append(np.array(pix,'float32'))\n",
        "        Y_train.append(row['emotion'])\n",
        "\n",
        "#print(X_train)\n",
        "X_train = np.array(X_train,'float32')\n",
        "Y_train = np.array(Y_train,'float32')\n",
        "X_test = np.array(X_test,'float32')\n",
        "Y_test = np.array(Y_test,'float32')\n",
        "\n",
        "print(X_train.shape)\n",
        "\n",
        "Y_train = np_utils.to_categorical(Y_train, num_classes=7)\n",
        "Y_test = np_utils.to_categorical(Y_test, num_classes=7)\n",
        "\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], 48, 48, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 48, 48, 1)\n",
        "\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "print(X_train.shape)\n",
        "\n",
        "#CNN model\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(64, kernel_size = (3,3), activation = 'relu', input_shape = (48,48,1), kernel_regularizer=l2(0.01)))\n",
        "model.add(Conv2D(64,kernel_size = (3,3), activation = 'relu', padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(128, kernel_size = (3,3), activation = 'relu', padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128, kernel_size = (3,3), activation = 'relu', padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128, kernel_size = (3,3), activation = 'relu', padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(256, kernel_size = (3,3), activation = 'relu',padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(256, kernel_size = (3,3), activation = 'relu',padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(256, kernel_size = (3,3), activation = 'relu',padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(512, kernel_size = (3,3), activation = 'relu',padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(512, kernel_size = (3,3), activation = 'relu',padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(512, kernel_size = (3,3), activation = 'relu',padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512,activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(256,activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(128,activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(64,activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(7,activation='softmax'))\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3)\n",
        "early_stopper = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=6, mode='auto')\n",
        "checkpointer = ModelCheckpoint('/content/drive/MyDrive/Expression recognition/needed/weights.hd5', monitor='val_loss', verbose=1, save_best_only=True)\n",
        "\n",
        "model.compile(loss = categorical_crossentropy, optimizer=Adam(lr=0.001), metrics=['accuracy'] )\n",
        "\n",
        "\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center=True,\n",
        "    featurewise_std_normalization=True,\n",
        "    rotation_range=30,width_shift_range=0.2, height_shift_range=0.2, vertical_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    )\n",
        "\n",
        "datagen.fit(X_train)\n",
        "\n",
        "train_generator = datagen.flow(X_train, Y_train, batch_size=64)\n",
        "\n",
        "#validation_generator = datagen.flow(X_train, y_train, batch_size=60, subset='validation')\n",
        "\n",
        "\n",
        "print(\"Training model\")\n",
        "# fits the model on batches with real-time data augmentation:\n",
        "model.fit_generator(generator=train_generator,\n",
        "                    validation_data=(X_test,Y_test),\n",
        "                    use_multiprocessing=True,\n",
        "                    steps_per_epoch = len(train_generator) / 64,\n",
        "                    epochs = 500)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#model.fit(X_train, Y_train, batch_size=64, epochs = 50, verbose=1, validation_data=(X_test,Y_test), shuffle = True, callbacks=[lr_reducer, checkpointer, early_stopper] )\n",
        "\n",
        "\n",
        "model_json = model.to_json()\n",
        "with open(\"model.json\",'w') as json_file:\n",
        "    json_file.write(model_json)\n",
        "print(\"Model saved\")\n",
        "model.save_weights(\"weights.h5\")\n",
        "print(\"Weights saved\")\n",
        "\n",
        "Y_pred = model.predict(X_test)\n",
        "Y_pred = np.argmax(Y_pred, axis=-1)\n",
        "#Y_test = Y_test.to_numpy()\n",
        "Y_test.flatten()\n",
        "Y_test.shape\n",
        "from sklearn.metrics import accuracy_score\n",
        "Y_pred = np_utils.to_categorical(Y_pred, num_classes=7)\n",
        "print(accuracy_score(Y_test, Y_pred))\n",
        "\n",
        "# accuracy = 0.2449150181108944\n",
        "# loss: 1.8367 - accuracy: 0.2217 - val_loss: 1.8183 - val_accuracy: 0.2449"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9La8DO2k4CB"
      },
      "source": [
        "#another method of data augmentation functions to improve accuracy\n",
        "import skimage as sk\n",
        "from skimage import transform\n",
        "from skimage import util\n",
        "from skimage import io\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.transform import AffineTransform\n",
        "\n",
        "def random_rotation(image_array):\n",
        "    random_degree = random.uniform(-25, 25)\n",
        "    return sk.transform.rotate(image_array, random_degree)\n",
        "\n",
        "def random_noise(img):\n",
        "    '''noise_img = sk.util.random_noise(image_array,var=0.1**2)\n",
        "    noise_img = np.array(255*noise_img, dtype = 'uint8')\n",
        "    return noise_img'''\n",
        "    #print(\"noisy\")\n",
        "    mean = 2.0   # some constant\n",
        "    std = 5.0   # some constant (standard deviation)\n",
        "    noisy_img = img + np.random.normal(mean, std, img.shape)\n",
        "    noisy_img_clipped = np.clip(noisy_img, 0, 255)\n",
        "    return noisy_img_clipped\n",
        "\n",
        "def horizontal_flip(image_array):\n",
        "    return image_array[:, ::-1]\n",
        "\n",
        "def sheared(image_array):\n",
        "  tf = AffineTransform(shear=-0.2)\n",
        "  sheared = transform.warp(image_array, tf, order=1, preserve_range=True, mode='wrap')\n",
        "  return sheared"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFzy-P1_NjvY",
        "outputId": "fd47e4c4-db04-4637-f408-470179f3543d"
      },
      "source": [
        "#model 3 - PublicTest + Training data used for training, PrivateTest for validation\n",
        "# also using data augmentation\n",
        "\n",
        "#importing required libraries\n",
        "import sys\n",
        "import os\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "\n",
        "# deep learning libraries\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Activation, Flatten, BatchNormalization\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, TensorBoard, ModelCheckpoint\n",
        "\n",
        "#load dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Expression recognition/needed/fer2013.csv\")\n",
        "\n",
        "#data visualization\n",
        "print(df.info())\n",
        "print(df.columns)\n",
        "print(df.head())\n",
        "print(df.Usage.unique())\n",
        "\n",
        "#check individual value counts\n",
        "temp = df[df['Usage'] != 'PrivateTest']\n",
        "print(temp['emotion'].value_counts())\n",
        "\n",
        "#save dataframe for further manipulation\n",
        "Y_df = temp['emotion']\n",
        "\n",
        "\n",
        "#data splitting\n",
        "X_train = []\n",
        "Y_train = []\n",
        "X_test = []\n",
        "Y_test = []\n",
        "\n",
        "for index,row in df.iterrows():\n",
        "    pix = row['pixels'].split(' ')\n",
        "    if row['Usage'] == 'PrivateTest':\n",
        "        X_test.append(np.array(pix,'float32'))\n",
        "        Y_test.append(row['emotion'])\n",
        "    else:\n",
        "        X_train.append(np.array(pix,'float32'))\n",
        "        Y_train.append(row['emotion'])\n",
        "\n",
        "#convert to numpy array\n",
        "X_train = np.array(X_train,'float32')\n",
        "Y_train = np.array(Y_train,'float32')\n",
        "X_test = np.array(X_test,'float32')\n",
        "Y_test = np.array(Y_test,'float32')\n",
        "\n",
        "print(X_train.shape)\n",
        "\n",
        "# reshape to 2D\n",
        "X_train = X_train.reshape(X_train.shape[0], 48, 48, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 48, 48, 1)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "\n",
        "\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 35887 entries, 0 to 35886\n",
            "Data columns (total 3 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   emotion  35887 non-null  int64 \n",
            " 1   pixels   35887 non-null  object\n",
            " 2   Usage    35887 non-null  object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 841.2+ KB\n",
            "None\n",
            "Index(['emotion', 'pixels', 'Usage'], dtype='object')\n",
            "   emotion                                             pixels     Usage\n",
            "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
            "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
            "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
            "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
            "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training\n",
            "['Training' 'PublicTest' 'PrivateTest']\n",
            "3    8110\n",
            "6    5572\n",
            "4    5483\n",
            "2    4593\n",
            "0    4462\n",
            "5    3586\n",
            "1     492\n",
            "Name: emotion, dtype: int64\n",
            "(32298, 2304)\n",
            "(32298, 48, 48, 1)\n",
            "(32298,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "id": "JaGA6cOUlSzW",
        "outputId": "5a2b2644-d26e-40ce-cbb7-c7b0531ff3f4"
      },
      "source": [
        "#we see that the the data values are highly incomparable\n",
        "#so we use data augmentation to add more data points for certain values\n",
        "\n",
        "#test data augmentation\n",
        "import random\n",
        "import cv2\n",
        "test_image = X_train[0].reshape(48,48)\n",
        "plt.imshow(test_image)\n",
        "\n",
        "transdict = {\n",
        "    'rotate': random_rotation,\n",
        "    'noise': random_noise,\n",
        "    'horizontal_flip': horizontal_flip,\n",
        "    'sheared' : sheared\n",
        "    }\n",
        "\n",
        "transformed_image = None\n",
        "key = random.choice(list(transdict))\n",
        "transformed_image = transdict[key](test_image)\n",
        "transformed_image = cv2.resize(transformed_image, (48,48))\n",
        "print(type(transformed_image))\n",
        "plt.imshow(transformed_image)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f4289ec4e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dDNi_mx5Hdn"
      },
      "source": [
        "# we increase data elements for values 2,0,5 and 1\n",
        "index2 = Y_df[Y_df == 2].index\n",
        "index0 = Y_df[Y_df == 0].index\n",
        "index5 = Y_df[Y_df == 5].index\n",
        "index1 = Y_df[Y_df == 1].index\n",
        "\n",
        "# adding augmented images for emotion 2\n",
        "for i in index2:\n",
        "  x = X_train[i].reshape(48,48)\n",
        "  transformed_image = None\n",
        "  key = random.choice(list(transdict))\n",
        "  transformed_image = transdict[key](test_image)\n",
        "  transformed_image = cv2.resize(transformed_image, (48,48))\n",
        "  final_img = transformed_image.reshape(1,48,48,1)\n",
        "  X_train = np.append(X_train,final_img,axis=0)\n",
        "\n",
        "  print(X_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BgM01QMuM7u"
      },
      "source": [
        "# adding augmented images for emotion 0\n",
        "for i in index0:\n",
        "  x = X_train[i].reshape(48,48)\n",
        "  transformed_image = None\n",
        "  key = random.choice(list(transdict))\n",
        "  transformed_image = transdict[key](test_image)\n",
        "  transformed_image = cv2.resize(transformed_image, (48,48))\n",
        "  final_img = transformed_image.reshape(1,48,48,1)\n",
        "  X_train = np.append(X_train,final_img,axis=0)\n",
        "  print(X_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2TEDGjjv5X8"
      },
      "source": [
        "# adding augmented images for emotion 5\n",
        "for i in index5:\n",
        "  x = X_train[i].reshape(48,48)\n",
        "  transformed_image = None\n",
        "  key = random.choice(list(transdict))\n",
        "  transformed_image = transdict[key](test_image)\n",
        "  transformed_image = cv2.resize(transformed_image, (48,48))\n",
        "  final_img = transformed_image.reshape(1,48,48,1)\n",
        "  X_train = np.append(X_train,final_img,axis=0)\n",
        "  print(X_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izpO7a9wxX7i"
      },
      "source": [
        "# adding augmented images for emotion 1\n",
        "for j in range(20):\n",
        "  for i in index1:\n",
        "    x = X_train[i].reshape(48,48)\n",
        "    transformed_image = None\n",
        "    key = random.choice(list(transdict))\n",
        "    transformed_image = transdict[key](test_image)\n",
        "    transformed_image = cv2.resize(transformed_image, (48,48))\n",
        "    final_img = transformed_image.reshape(1,48,48,1)\n",
        "    X_train = np.append(X_train,final_img,axis=0)\n",
        "    print(X_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGPbsaVg7GAu",
        "outputId": "9d2734ea-56ce-4300-c0a0-5dbb2ceb238f"
      },
      "source": [
        "print(X_train.shape)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(54779, 48, 48, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        },
        "id": "4THUGg0mx4BS",
        "outputId": "0bdf8a49-e9d9-42bd-cbe5-a9723c661643"
      },
      "source": [
        "#adding corresponding values to Y_train\n",
        "Y_train = np.append(Y_train,np.full((4593,),2))\n",
        "print(Y_train.shape)\n",
        "Y_train = np.append(Y_train,np.full((4462,),0))\n",
        "print(Y_train.shape)\n",
        "Y_train = np.append(Y_train,np.full((3586,),5))\n",
        "print(Y_train.shape)\n",
        "Y_train = np.append(Y_train,np.full((9840,),1))\n",
        "print(Y_train.shape)\n",
        "#print(Y_train.value_counts())"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(36891,)\n",
            "(41353,)\n",
            "(44939,)\n",
            "(54779,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-e58a7c377255>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mY_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9840\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'value_counts'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exe6a99a5dxv"
      },
      "source": [
        "# convert to one-hot encoding\n",
        "Y_train = np_utils.to_categorical(Y_train, num_classes=7)\n",
        "Y_test = np_utils.to_categorical(Y_test, num_classes=7)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgfjkqs7i4Sx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0641d79-b584-47be-b4cf-f696395c11a1"
      },
      "source": [
        "#CNN model\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(64, kernel_size = (3,3), activation = 'relu', input_shape = (48,48,1), kernel_regularizer=l2(0.01)))\n",
        "model.add(Conv2D(64,kernel_size = (3,3), activation = 'relu', padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(128, kernel_size = (3,3), activation = 'relu', padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128, kernel_size = (3,3), activation = 'relu', padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128, kernel_size = (3,3), activation = 'relu', padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(256, kernel_size = (3,3), activation = 'relu',padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(256, kernel_size = (3,3), activation = 'relu',padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(256, kernel_size = (3,3), activation = 'relu',padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(512, kernel_size = (3,3), activation = 'relu',padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(512, kernel_size = (3,3), activation = 'relu',padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(512, kernel_size = (3,3), activation = 'relu',padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512,activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(256,activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(128,activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(64,activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(7,activation='softmax'))\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3)\n",
        "early_stopper = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=6, mode='auto')\n",
        "checkpointer = ModelCheckpoint('/content/drive/MyDrive/Expression recognition/needed/weights.hd5', monitor='val_loss', verbose=1, save_best_only=True)\n",
        "\n",
        "model.compile(loss = categorical_crossentropy, optimizer=Adam(lr=0.001), metrics=['accuracy'] )\n",
        "\n",
        "print(\"Training model\")\n",
        "model.fit(X_train, Y_train, batch_size=64, epochs = 100, verbose=1, validation_data=(X_test,Y_test), shuffle = True, callbacks=[lr_reducer, checkpointer, early_stopper] )\n",
        "\n",
        "\n",
        "model_json = model.to_json()\n",
        "with open(\"model.json\",'w') as json_file:\n",
        "    json_file.write(model_json)\n",
        "print(\"Model saved\")\n",
        "model.save_weights(\"weights.h5\")\n",
        "print(\"Weights saved\")\n",
        "\n",
        "Y_pred = model.predict(X_test)\n",
        "Y_pred = np.argmax(Y_pred, axis=-1)\n",
        "#Y_test = Y_test.to_numpy()\n",
        "Y_test.flatten()\n",
        "Y_test.shape\n",
        "from sklearn.metrics import accuracy_score\n",
        "Y_pred = np_utils.to_categorical(Y_pred, num_classes=7)\n",
        "print(accuracy_score(Y_test, Y_pred))\n",
        "\n",
        "# accuracy = 0.6224575090554472\n",
        "# loss: 0.8928 - accuracy: 0.6495 - val_loss: 1.3426 - val_accuracy: 0.6225"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model\n",
            "Epoch 1/100\n",
            "856/856 [==============================] - 31s 34ms/step - loss: 2.3035 - accuracy: 0.1964 - val_loss: 2.0175 - val_accuracy: 0.2288\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 2.01745, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 2/100\n",
            "856/856 [==============================] - 29s 33ms/step - loss: 1.6676 - accuracy: 0.3207 - val_loss: 2.0736 - val_accuracy: 0.2301\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 2.01745\n",
            "Epoch 3/100\n",
            "856/856 [==============================] - 29s 33ms/step - loss: 1.6493 - accuracy: 0.3255 - val_loss: 1.8307 - val_accuracy: 0.2458\n",
            "\n",
            "Epoch 00003: val_loss improved from 2.01745 to 1.83068, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 4/100\n",
            "856/856 [==============================] - 29s 33ms/step - loss: 1.6381 - accuracy: 0.3271 - val_loss: 1.8416 - val_accuracy: 0.2432\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 1.83068\n",
            "Epoch 5/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.6276 - accuracy: 0.3271 - val_loss: 1.9008 - val_accuracy: 0.2405\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 1.83068\n",
            "Epoch 6/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.6313 - accuracy: 0.3200 - val_loss: 1.8155 - val_accuracy: 0.2491\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.83068 to 1.81551, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 7/100\n",
            "856/856 [==============================] - 29s 33ms/step - loss: 1.6211 - accuracy: 0.3278 - val_loss: 1.8281 - val_accuracy: 0.2438\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 1.81551\n",
            "Epoch 8/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.6230 - accuracy: 0.3301 - val_loss: 1.8069 - val_accuracy: 0.2449\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.81551 to 1.80689, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 9/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.6211 - accuracy: 0.3288 - val_loss: 1.7966 - val_accuracy: 0.2449\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.80689 to 1.79658, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 10/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.6185 - accuracy: 0.3265 - val_loss: 1.8194 - val_accuracy: 0.2441\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 1.79658\n",
            "Epoch 11/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.6045 - accuracy: 0.3353 - val_loss: 1.7765 - val_accuracy: 0.2636\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.79658 to 1.77648, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 12/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.5872 - accuracy: 0.3457 - val_loss: 1.7276 - val_accuracy: 0.2895\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.77648 to 1.72759, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 13/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.5773 - accuracy: 0.3461 - val_loss: 1.7019 - val_accuracy: 0.3040\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.72759 to 1.70190, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 14/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.5664 - accuracy: 0.3541 - val_loss: 1.7526 - val_accuracy: 0.2834\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 1.70190\n",
            "Epoch 15/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.5603 - accuracy: 0.3552 - val_loss: 1.7203 - val_accuracy: 0.2792\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 1.70190\n",
            "Epoch 16/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.5205 - accuracy: 0.3786 - val_loss: 1.5655 - val_accuracy: 0.3631\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.70190 to 1.56547, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 17/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.4899 - accuracy: 0.3910 - val_loss: 1.6314 - val_accuracy: 0.3502\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 1.56547\n",
            "Epoch 18/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.4459 - accuracy: 0.4121 - val_loss: 1.4439 - val_accuracy: 0.4154\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.56547 to 1.44386, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 19/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.4287 - accuracy: 0.4177 - val_loss: 1.4432 - val_accuracy: 0.4154\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.44386 to 1.44323, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 20/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.3995 - accuracy: 0.4290 - val_loss: 1.4927 - val_accuracy: 0.3918\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 1.44323\n",
            "Epoch 21/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.3915 - accuracy: 0.4274 - val_loss: 1.3649 - val_accuracy: 0.4436\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.44323 to 1.36490, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 22/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.3741 - accuracy: 0.4326 - val_loss: 1.3489 - val_accuracy: 0.4324\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.36490 to 1.34887, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 23/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.3593 - accuracy: 0.4407 - val_loss: 1.4830 - val_accuracy: 0.4009\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 1.34887\n",
            "Epoch 24/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.3443 - accuracy: 0.4418 - val_loss: 1.3256 - val_accuracy: 0.4419\n",
            "\n",
            "Epoch 00024: val_loss improved from 1.34887 to 1.32559, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 25/100\n",
            "856/856 [==============================] - 29s 33ms/step - loss: 1.3412 - accuracy: 0.4394 - val_loss: 1.3094 - val_accuracy: 0.4597\n",
            "\n",
            "Epoch 00025: val_loss improved from 1.32559 to 1.30943, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 26/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.3182 - accuracy: 0.4542 - val_loss: 1.3793 - val_accuracy: 0.4361\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 1.30943\n",
            "Epoch 27/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.3162 - accuracy: 0.4617 - val_loss: 1.3178 - val_accuracy: 0.4776\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 1.30943\n",
            "Epoch 28/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.3014 - accuracy: 0.4688 - val_loss: 1.2684 - val_accuracy: 0.5104\n",
            "\n",
            "Epoch 00028: val_loss improved from 1.30943 to 1.26843, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 29/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.3009 - accuracy: 0.4741 - val_loss: 1.3403 - val_accuracy: 0.4765\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 1.26843\n",
            "Epoch 30/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.2794 - accuracy: 0.4805 - val_loss: 1.2476 - val_accuracy: 0.5177\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.26843 to 1.24761, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 31/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.2615 - accuracy: 0.4888 - val_loss: 1.2615 - val_accuracy: 0.5222\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 1.24761\n",
            "Epoch 32/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.2574 - accuracy: 0.4926 - val_loss: 1.2459 - val_accuracy: 0.5188\n",
            "\n",
            "Epoch 00032: val_loss improved from 1.24761 to 1.24591, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 33/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.2517 - accuracy: 0.4974 - val_loss: 1.2191 - val_accuracy: 0.5350\n",
            "\n",
            "Epoch 00033: val_loss improved from 1.24591 to 1.21911, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 34/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.2424 - accuracy: 0.5019 - val_loss: 1.1708 - val_accuracy: 0.5497\n",
            "\n",
            "Epoch 00034: val_loss improved from 1.21911 to 1.17082, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 35/100\n",
            "856/856 [==============================] - 29s 33ms/step - loss: 1.2381 - accuracy: 0.5082 - val_loss: 1.2260 - val_accuracy: 0.5411\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 1.17082\n",
            "Epoch 36/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.2133 - accuracy: 0.5172 - val_loss: 1.1871 - val_accuracy: 0.5617\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 1.17082\n",
            "Epoch 37/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.2130 - accuracy: 0.5143 - val_loss: 1.1710 - val_accuracy: 0.5626\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 1.17082\n",
            "Epoch 38/100\n",
            "856/856 [==============================] - 29s 33ms/step - loss: 1.1908 - accuracy: 0.5254 - val_loss: 1.2615 - val_accuracy: 0.5210\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 1.17082\n",
            "Epoch 39/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.1926 - accuracy: 0.5229 - val_loss: 1.1358 - val_accuracy: 0.5798\n",
            "\n",
            "Epoch 00039: val_loss improved from 1.17082 to 1.13580, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 40/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.1771 - accuracy: 0.5323 - val_loss: 1.1492 - val_accuracy: 0.5717\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 1.13580\n",
            "Epoch 41/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.1648 - accuracy: 0.5369 - val_loss: 1.1463 - val_accuracy: 0.5651\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 1.13580\n",
            "Epoch 42/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.1657 - accuracy: 0.5392 - val_loss: 1.1266 - val_accuracy: 0.5798\n",
            "\n",
            "Epoch 00042: val_loss improved from 1.13580 to 1.12661, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 43/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.1597 - accuracy: 0.5382 - val_loss: 1.1889 - val_accuracy: 0.5695\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 1.12661\n",
            "Epoch 44/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.1497 - accuracy: 0.5421 - val_loss: 1.1178 - val_accuracy: 0.5882\n",
            "\n",
            "Epoch 00044: val_loss improved from 1.12661 to 1.11782, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 45/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.1329 - accuracy: 0.5498 - val_loss: 1.1304 - val_accuracy: 0.5804\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 1.11782\n",
            "Epoch 46/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.1369 - accuracy: 0.5459 - val_loss: 1.1577 - val_accuracy: 0.5687\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 1.11782\n",
            "Epoch 47/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.1387 - accuracy: 0.5495 - val_loss: 1.2276 - val_accuracy: 0.5467\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 1.11782\n",
            "Epoch 48/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.1241 - accuracy: 0.5532 - val_loss: 1.1636 - val_accuracy: 0.5890\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 1.11782\n",
            "Epoch 49/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.1071 - accuracy: 0.5601 - val_loss: 1.1445 - val_accuracy: 0.5946\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 1.11782\n",
            "Epoch 50/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.0998 - accuracy: 0.5645 - val_loss: 1.1825 - val_accuracy: 0.5756\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 1.11782\n",
            "Epoch 51/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.0889 - accuracy: 0.5706 - val_loss: 1.1430 - val_accuracy: 0.5851\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 1.11782\n",
            "Epoch 52/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.0784 - accuracy: 0.5749 - val_loss: 1.1349 - val_accuracy: 0.5954\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 1.11782\n",
            "Epoch 53/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.0689 - accuracy: 0.5783 - val_loss: 1.1434 - val_accuracy: 0.6007\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 1.11782\n",
            "Epoch 54/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.0559 - accuracy: 0.5833 - val_loss: 1.1976 - val_accuracy: 0.5935\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 1.11782\n",
            "Epoch 55/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.0580 - accuracy: 0.5852 - val_loss: 1.1699 - val_accuracy: 0.6133\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 1.11782\n",
            "Epoch 56/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.0457 - accuracy: 0.5893 - val_loss: 1.2514 - val_accuracy: 0.5759\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 1.11782\n",
            "Epoch 57/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.0371 - accuracy: 0.5931 - val_loss: 1.2006 - val_accuracy: 0.5901\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 1.11782\n",
            "Epoch 58/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.0309 - accuracy: 0.5905 - val_loss: 1.1882 - val_accuracy: 0.6032\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 1.11782\n",
            "Epoch 59/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.0222 - accuracy: 0.5960 - val_loss: 1.1672 - val_accuracy: 0.6144\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 1.11782\n",
            "Epoch 60/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.0066 - accuracy: 0.6019 - val_loss: 1.1323 - val_accuracy: 0.6066\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 1.11782\n",
            "Epoch 61/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.0050 - accuracy: 0.6038 - val_loss: 1.1805 - val_accuracy: 0.6177\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 1.11782\n",
            "Epoch 62/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 0.9972 - accuracy: 0.6055 - val_loss: 1.3151 - val_accuracy: 0.5829\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 1.11782\n",
            "Epoch 63/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.0005 - accuracy: 0.6034 - val_loss: 1.1653 - val_accuracy: 0.6219\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 1.11782\n",
            "Epoch 64/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 0.9861 - accuracy: 0.6114 - val_loss: 1.2501 - val_accuracy: 0.6108\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 1.11782\n",
            "Epoch 65/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 0.9665 - accuracy: 0.6203 - val_loss: 1.2380 - val_accuracy: 0.6180\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 1.11782\n",
            "Epoch 66/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 0.9705 - accuracy: 0.6173 - val_loss: 1.2251 - val_accuracy: 0.6141\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 1.11782\n",
            "Epoch 67/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 0.9545 - accuracy: 0.6241 - val_loss: 1.1921 - val_accuracy: 0.6152\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 1.11782\n",
            "Epoch 68/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 0.9498 - accuracy: 0.6213 - val_loss: 1.1629 - val_accuracy: 0.6208\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 1.11782\n",
            "Epoch 69/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 0.9406 - accuracy: 0.6263 - val_loss: 1.2495 - val_accuracy: 0.6255\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 1.11782\n",
            "Epoch 70/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 0.9340 - accuracy: 0.6331 - val_loss: 1.3009 - val_accuracy: 0.6202\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 1.11782\n",
            "Epoch 71/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 0.9338 - accuracy: 0.6316 - val_loss: 1.2456 - val_accuracy: 0.6225\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 1.11782\n",
            "Epoch 72/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 0.9237 - accuracy: 0.6340 - val_loss: 1.2339 - val_accuracy: 0.6317\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 1.11782\n",
            "Epoch 73/100\n",
            "856/856 [==============================] - 29s 33ms/step - loss: 0.9205 - accuracy: 0.6369 - val_loss: 1.2966 - val_accuracy: 0.6169\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 1.11782\n",
            "Epoch 74/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 0.9204 - accuracy: 0.6349 - val_loss: 1.3818 - val_accuracy: 0.6227\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 1.11782\n",
            "Epoch 75/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 0.9183 - accuracy: 0.6410 - val_loss: 1.3230 - val_accuracy: 0.6297\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 1.11782\n",
            "Epoch 76/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 0.8988 - accuracy: 0.6424 - val_loss: 1.2983 - val_accuracy: 0.6269\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 1.11782\n",
            "Epoch 77/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 0.8947 - accuracy: 0.6454 - val_loss: 1.3264 - val_accuracy: 0.6272\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 1.11782\n",
            "Epoch 78/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 0.8928 - accuracy: 0.6495 - val_loss: 1.3426 - val_accuracy: 0.6225\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 1.11782\n",
            "Model saved\n",
            "Weights saved\n",
            "0.6224575090554472\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e75clw9ieez",
        "outputId": "1e4307c6-3876-454d-e068-69bf6919915e"
      },
      "source": [
        "#model 4 - decreasing the learning rate from 0.01 to 0.001 - best accuracy\n",
        "#CNN model\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(64, kernel_size = (3,3), activation = 'relu', input_shape = (48,48,1), kernel_regularizer=l2(0.001)))\n",
        "model.add(Conv2D(64,kernel_size = (3,3), activation = 'relu', padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(128, kernel_size = (3,3), activation = 'relu', padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128, kernel_size = (3,3), activation = 'relu', padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128, kernel_size = (3,3), activation = 'relu', padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(256, kernel_size = (3,3), activation = 'relu',padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(256, kernel_size = (3,3), activation = 'relu',padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(256, kernel_size = (3,3), activation = 'relu',padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(512, kernel_size = (3,3), activation = 'relu',padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(512, kernel_size = (3,3), activation = 'relu',padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(512, kernel_size = (3,3), activation = 'relu',padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512,activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(256,activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(128,activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(64,activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(7,activation='softmax'))\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3)\n",
        "early_stopper = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=6, mode='auto')\n",
        "checkpointer = ModelCheckpoint('/content/drive/MyDrive/Expression recognition/needed/weights.hd5', monitor='val_loss', verbose=1, save_best_only=True)\n",
        "\n",
        "model.compile(loss = categorical_crossentropy, optimizer=Adam(lr=0.001), metrics=['accuracy'] )\n",
        "\n",
        "print(\"Training model\")\n",
        "model.fit(X_train, Y_train, batch_size=64, epochs = 100, verbose=1, validation_data=(X_test,Y_test), shuffle = True, callbacks=[lr_reducer, checkpointer, early_stopper] )\n",
        "\n",
        "\n",
        "model_json = model.to_json()\n",
        "with open(\"model.json\",'w') as json_file:\n",
        "    json_file.write(model_json)\n",
        "print(\"Model saved\")\n",
        "model.save_weights(\"weights.h5\")\n",
        "print(\"Weights saved\")\n",
        "\n",
        "Y_pred = model.predict(X_test)\n",
        "Y_pred = np.argmax(Y_pred, axis=-1)\n",
        "#Y_test = Y_test.to_numpy()\n",
        "Y_test.flatten()\n",
        "Y_test.shape\n",
        "from sklearn.metrics import accuracy_score\n",
        "Y_pred = np_utils.to_categorical(Y_pred, num_classes=7)\n",
        "print(accuracy_score(Y_test, Y_pred))\n",
        "\n",
        "# accuracy = 0.6402897743103929\n",
        "# loss: 0.8406 - accuracy: 0.6644 - val_loss: 1.5589 - val_accuracy: 0.6403\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model\n",
            "Epoch 1/100\n",
            "856/856 [==============================] - 30s 34ms/step - loss: 2.2866 - accuracy: 0.2009 - val_loss: 1.9406 - val_accuracy: 0.2349\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.94062, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 2/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.6577 - accuracy: 0.3104 - val_loss: 1.8301 - val_accuracy: 0.2432\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.94062 to 1.83015, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 3/100\n",
            "856/856 [==============================] - 29s 33ms/step - loss: 1.6378 - accuracy: 0.3221 - val_loss: 1.8111 - val_accuracy: 0.2449\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.83015 to 1.81113, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 4/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.6235 - accuracy: 0.3222 - val_loss: 1.8908 - val_accuracy: 0.2449\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 1.81113\n",
            "Epoch 5/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.6313 - accuracy: 0.3259 - val_loss: 1.8255 - val_accuracy: 0.2449\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 1.81113\n",
            "Epoch 6/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.6249 - accuracy: 0.3215 - val_loss: 1.8088 - val_accuracy: 0.2446\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.81113 to 1.80883, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 7/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.6191 - accuracy: 0.3263 - val_loss: 1.8275 - val_accuracy: 0.2449\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 1.80883\n",
            "Epoch 8/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.6207 - accuracy: 0.3216 - val_loss: 1.7804 - val_accuracy: 0.2536\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.80883 to 1.78044, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 9/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.6063 - accuracy: 0.3309 - val_loss: 1.7783 - val_accuracy: 0.2600\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.78044 to 1.77828, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 10/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.5955 - accuracy: 0.3353 - val_loss: 1.7089 - val_accuracy: 0.2984\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.77828 to 1.70892, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 11/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.5663 - accuracy: 0.3505 - val_loss: 1.7038 - val_accuracy: 0.2942\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.70892 to 1.70378, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 12/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.5359 - accuracy: 0.3664 - val_loss: 1.6481 - val_accuracy: 0.3461\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.70378 to 1.64810, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 13/100\n",
            "856/856 [==============================] - 29s 34ms/step - loss: 1.4917 - accuracy: 0.3898 - val_loss: 1.6114 - val_accuracy: 0.3352\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.64810 to 1.61135, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 14/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.4543 - accuracy: 0.4054 - val_loss: 1.5003 - val_accuracy: 0.4001\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.61135 to 1.50032, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 15/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.4205 - accuracy: 0.4199 - val_loss: 1.4137 - val_accuracy: 0.4238\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.50032 to 1.41367, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 16/100\n",
            "856/856 [==============================] - 29s 33ms/step - loss: 1.4058 - accuracy: 0.4254 - val_loss: 1.3837 - val_accuracy: 0.4310\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.41367 to 1.38367, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 17/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.3779 - accuracy: 0.4327 - val_loss: 1.4035 - val_accuracy: 0.4263\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 1.38367\n",
            "Epoch 18/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.3742 - accuracy: 0.4318 - val_loss: 1.3402 - val_accuracy: 0.4480\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.38367 to 1.34018, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 19/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.3639 - accuracy: 0.4371 - val_loss: 1.3669 - val_accuracy: 0.4271\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 1.34018\n",
            "Epoch 20/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.3488 - accuracy: 0.4393 - val_loss: 1.3736 - val_accuracy: 0.4327\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 1.34018\n",
            "Epoch 21/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.3467 - accuracy: 0.4450 - val_loss: 1.3457 - val_accuracy: 0.4511\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 1.34018\n",
            "Epoch 22/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.3240 - accuracy: 0.4523 - val_loss: 1.3004 - val_accuracy: 0.4664\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.34018 to 1.30035, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 23/100\n",
            "856/856 [==============================] - 29s 33ms/step - loss: 1.3135 - accuracy: 0.4573 - val_loss: 1.2745 - val_accuracy: 0.4723\n",
            "\n",
            "Epoch 00023: val_loss improved from 1.30035 to 1.27447, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 24/100\n",
            "856/856 [==============================] - 29s 33ms/step - loss: 1.2976 - accuracy: 0.4653 - val_loss: 1.2552 - val_accuracy: 0.5060\n",
            "\n",
            "Epoch 00024: val_loss improved from 1.27447 to 1.25523, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 25/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.2923 - accuracy: 0.4725 - val_loss: 1.2700 - val_accuracy: 0.5040\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 1.25523\n",
            "Epoch 26/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.2877 - accuracy: 0.4761 - val_loss: 1.2262 - val_accuracy: 0.5085\n",
            "\n",
            "Epoch 00026: val_loss improved from 1.25523 to 1.22618, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 27/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.2648 - accuracy: 0.4854 - val_loss: 1.2190 - val_accuracy: 0.5283\n",
            "\n",
            "Epoch 00027: val_loss improved from 1.22618 to 1.21905, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 28/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.2665 - accuracy: 0.4831 - val_loss: 1.2063 - val_accuracy: 0.5283\n",
            "\n",
            "Epoch 00028: val_loss improved from 1.21905 to 1.20628, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 29/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.2401 - accuracy: 0.4980 - val_loss: 1.2336 - val_accuracy: 0.5160\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 1.20628\n",
            "Epoch 30/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.2324 - accuracy: 0.5028 - val_loss: 1.1670 - val_accuracy: 0.5595\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.20628 to 1.16698, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 31/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.2273 - accuracy: 0.5063 - val_loss: 1.2632 - val_accuracy: 0.4974\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 1.16698\n",
            "Epoch 32/100\n",
            "856/856 [==============================] - 29s 33ms/step - loss: 1.2235 - accuracy: 0.5107 - val_loss: 1.2044 - val_accuracy: 0.5350\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 1.16698\n",
            "Epoch 33/100\n",
            "856/856 [==============================] - 29s 34ms/step - loss: 1.2065 - accuracy: 0.5153 - val_loss: 1.1424 - val_accuracy: 0.5651\n",
            "\n",
            "Epoch 00033: val_loss improved from 1.16698 to 1.14244, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 34/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.1886 - accuracy: 0.5244 - val_loss: 1.1727 - val_accuracy: 0.5623\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 1.14244\n",
            "Epoch 35/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.1887 - accuracy: 0.5226 - val_loss: 1.1501 - val_accuracy: 0.5603\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 1.14244\n",
            "Epoch 36/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.1802 - accuracy: 0.5257 - val_loss: 1.1468 - val_accuracy: 0.5665\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 1.14244\n",
            "Epoch 37/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.1686 - accuracy: 0.5304 - val_loss: 1.1842 - val_accuracy: 0.5517\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 1.14244\n",
            "Epoch 38/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.1515 - accuracy: 0.5396 - val_loss: 1.1594 - val_accuracy: 0.5606\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 1.14244\n",
            "Epoch 39/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.1425 - accuracy: 0.5457 - val_loss: 1.1296 - val_accuracy: 0.5862\n",
            "\n",
            "Epoch 00039: val_loss improved from 1.14244 to 1.12957, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 40/100\n",
            "856/856 [==============================] - 29s 33ms/step - loss: 1.1239 - accuracy: 0.5512 - val_loss: 1.1383 - val_accuracy: 0.5887\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 1.12957\n",
            "Epoch 41/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.1268 - accuracy: 0.5523 - val_loss: 1.1779 - val_accuracy: 0.5848\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 1.12957\n",
            "Epoch 42/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.1255 - accuracy: 0.5538 - val_loss: 1.1225 - val_accuracy: 0.5935\n",
            "\n",
            "Epoch 00042: val_loss improved from 1.12957 to 1.12251, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 43/100\n",
            "856/856 [==============================] - 29s 33ms/step - loss: 1.1290 - accuracy: 0.5510 - val_loss: 1.1325 - val_accuracy: 0.5921\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 1.12251\n",
            "Epoch 44/100\n",
            "856/856 [==============================] - 29s 33ms/step - loss: 1.0957 - accuracy: 0.5654 - val_loss: 1.1496 - val_accuracy: 0.5890\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 1.12251\n",
            "Epoch 45/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.0969 - accuracy: 0.5634 - val_loss: 1.1531 - val_accuracy: 0.6004\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 1.12251\n",
            "Epoch 46/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.0944 - accuracy: 0.5640 - val_loss: 1.1278 - val_accuracy: 0.6055\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 1.12251\n",
            "Epoch 47/100\n",
            "856/856 [==============================] - 29s 34ms/step - loss: 1.0765 - accuracy: 0.5723 - val_loss: 1.1126 - val_accuracy: 0.6088\n",
            "\n",
            "Epoch 00047: val_loss improved from 1.12251 to 1.11265, saving model to /content/drive/MyDrive/Expression recognition/needed/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Expression recognition/needed/weights.hd5/assets\n",
            "Epoch 48/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.0677 - accuracy: 0.5736 - val_loss: 1.1197 - val_accuracy: 0.6133\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 1.11265\n",
            "Epoch 49/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.0678 - accuracy: 0.5775 - val_loss: 1.1410 - val_accuracy: 0.6152\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 1.11265\n",
            "Epoch 50/100\n",
            "856/856 [==============================] - 29s 33ms/step - loss: 1.0530 - accuracy: 0.5815 - val_loss: 1.1150 - val_accuracy: 0.6186\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 1.11265\n",
            "Epoch 51/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.0460 - accuracy: 0.5850 - val_loss: 1.1640 - val_accuracy: 0.6147\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 1.11265\n",
            "Epoch 52/100\n",
            "856/856 [==============================] - 29s 33ms/step - loss: 1.0434 - accuracy: 0.5897 - val_loss: 1.1704 - val_accuracy: 0.6174\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 1.11265\n",
            "Epoch 53/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.0339 - accuracy: 0.5934 - val_loss: 1.1308 - val_accuracy: 0.6174\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 1.11265\n",
            "Epoch 54/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.0273 - accuracy: 0.5950 - val_loss: 1.2017 - val_accuracy: 0.6194\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 1.11265\n",
            "Epoch 55/100\n",
            "856/856 [==============================] - 29s 33ms/step - loss: 1.0135 - accuracy: 0.6005 - val_loss: 1.1684 - val_accuracy: 0.6266\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 1.11265\n",
            "Epoch 56/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.0073 - accuracy: 0.6011 - val_loss: 1.2425 - val_accuracy: 0.6269\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 1.11265\n",
            "Epoch 57/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 1.0028 - accuracy: 0.6058 - val_loss: 1.1582 - val_accuracy: 0.6252\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 1.11265\n",
            "Epoch 58/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 0.9915 - accuracy: 0.6106 - val_loss: 1.1683 - val_accuracy: 0.6258\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 1.11265\n",
            "Epoch 59/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 0.9683 - accuracy: 0.6188 - val_loss: 1.1595 - val_accuracy: 0.6294\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 1.11265\n",
            "Epoch 60/100\n",
            "856/856 [==============================] - 29s 33ms/step - loss: 0.9738 - accuracy: 0.6161 - val_loss: 1.1915 - val_accuracy: 0.6255\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 1.11265\n",
            "Epoch 61/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 0.9705 - accuracy: 0.6184 - val_loss: 1.2805 - val_accuracy: 0.6300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 1.11265\n",
            "Epoch 62/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 0.9540 - accuracy: 0.6218 - val_loss: 1.2325 - val_accuracy: 0.6325\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 1.11265\n",
            "Epoch 63/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 0.9480 - accuracy: 0.6243 - val_loss: 1.2748 - val_accuracy: 0.6261\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 1.11265\n",
            "Epoch 64/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 0.9413 - accuracy: 0.6275 - val_loss: 1.2927 - val_accuracy: 0.6319\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 1.11265\n",
            "Epoch 65/100\n",
            "856/856 [==============================] - 29s 33ms/step - loss: 0.9284 - accuracy: 0.6326 - val_loss: 1.2549 - val_accuracy: 0.6339\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 1.11265\n",
            "Epoch 66/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 0.9236 - accuracy: 0.6337 - val_loss: 1.2440 - val_accuracy: 0.6294\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 1.11265\n",
            "Epoch 67/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 0.9144 - accuracy: 0.6385 - val_loss: 1.2331 - val_accuracy: 0.6225\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 1.11265\n",
            "Epoch 68/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 0.9192 - accuracy: 0.6374 - val_loss: 1.2860 - val_accuracy: 0.6364\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 1.11265\n",
            "Epoch 69/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 0.9117 - accuracy: 0.6416 - val_loss: 1.3311 - val_accuracy: 0.6322\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 1.11265\n",
            "Epoch 70/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 0.8953 - accuracy: 0.6431 - val_loss: 1.2684 - val_accuracy: 0.6383\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 1.11265\n",
            "Epoch 71/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 0.8965 - accuracy: 0.6455 - val_loss: 1.4330 - val_accuracy: 0.6272\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 1.11265\n",
            "Epoch 72/100\n",
            "856/856 [==============================] - 29s 33ms/step - loss: 0.8855 - accuracy: 0.6517 - val_loss: 1.4208 - val_accuracy: 0.6411\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 1.11265\n",
            "Epoch 73/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 0.8836 - accuracy: 0.6544 - val_loss: 1.4291 - val_accuracy: 0.6364\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 1.11265\n",
            "Epoch 74/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 0.8758 - accuracy: 0.6551 - val_loss: 1.4026 - val_accuracy: 0.6356\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 1.11265\n",
            "Epoch 75/100\n",
            "856/856 [==============================] - 29s 33ms/step - loss: 0.8710 - accuracy: 0.6571 - val_loss: 1.4681 - val_accuracy: 0.6400\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 1.11265\n",
            "Epoch 76/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 0.8652 - accuracy: 0.6554 - val_loss: 1.4452 - val_accuracy: 0.6372\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 1.11265\n",
            "Epoch 77/100\n",
            "856/856 [==============================] - 29s 33ms/step - loss: 0.8626 - accuracy: 0.6594 - val_loss: 1.4482 - val_accuracy: 0.6417\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 1.11265\n",
            "Epoch 78/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 0.8559 - accuracy: 0.6628 - val_loss: 1.4707 - val_accuracy: 0.6411\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 1.11265\n",
            "Epoch 79/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 0.8477 - accuracy: 0.6627 - val_loss: 1.4189 - val_accuracy: 0.6397\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 1.11265\n",
            "Epoch 80/100\n",
            "856/856 [==============================] - 29s 33ms/step - loss: 0.8523 - accuracy: 0.6611 - val_loss: 1.4575 - val_accuracy: 0.6395\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 1.11265\n",
            "Epoch 81/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 0.8374 - accuracy: 0.6686 - val_loss: 1.4504 - val_accuracy: 0.6406\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 1.11265\n",
            "Epoch 82/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 0.8364 - accuracy: 0.6674 - val_loss: 1.4975 - val_accuracy: 0.6367\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 1.11265\n",
            "Epoch 83/100\n",
            "856/856 [==============================] - 28s 33ms/step - loss: 0.8406 - accuracy: 0.6644 - val_loss: 1.5589 - val_accuracy: 0.6403\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 1.11265\n",
            "Model saved\n",
            "Weights saved\n",
            "0.6402897743103929\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aF9heFEFze_"
      },
      "source": [
        "\n",
        "# shallow model - tested previously\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "g_giLYswGEtq",
        "outputId": "02999101-e66f-4fbe-cd63-570697955453"
      },
      "source": [
        "#loading data\n",
        "path_to_data = '/content/drive/MyDrive/Expression recognition/needed/'\n",
        "data = pd.read_csv(path_to_data+'icml_face_data.csv')\n",
        "\n",
        "data.head()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>Usage</th>\n",
              "      <th>pixels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Training</td>\n",
              "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Training</td>\n",
              "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Training</td>\n",
              "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Training</td>\n",
              "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>Training</td>\n",
              "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   emotion     Usage                                             pixels\n",
              "0        0  Training  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...\n",
              "1        0  Training  151 150 147 155 148 133 111 140 170 174 182 15...\n",
              "2        2  Training  231 212 156 164 174 138 161 173 182 200 106 38...\n",
              "3        4  Training  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...\n",
              "4        6  Training  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mz8jnNhdISt8",
        "outputId": "d45779bf-01cf-4d7a-88d5-f68ec6751d85"
      },
      "source": [
        "data.tail()\n",
        "data.columns\n",
        "data = data.rename(columns={' Usage': 'Usage'})\n",
        "data = data.rename(columns={' pixels': 'pixels'})\n",
        "\n",
        "data.columns\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['emotion', 'Usage', 'pixels'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "oRZU19NAIYiK",
        "outputId": "0c36616a-38d8-4256-eead-01564ba4cad6"
      },
      "source": [
        "emotion = {0:'Angry',1:'Disgust',2:'Fear',3:'Happy',4:'Sad',5:'Surprise',6:'Neutral'}\n",
        "\n",
        "x_train = data.loc[data.Usage == 'Training',['pixels']]\n",
        "y_train = data.loc[data.Usage == 'Training',['emotion']]\n",
        "x_test = data.loc[data.Usage != 'Training',['pixels']]\n",
        "y_test = data.loc[data.Usage != 'Training',['emotion']]\n",
        "x_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pixels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28704</th>\n",
              "      <td>84 85 85 85 85 85 85 85 86 86 86 87 86 86 91 9...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28705</th>\n",
              "      <td>114 112 113 113 111 111 112 113 115 113 114 11...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28706</th>\n",
              "      <td>74 81 87 89 95 100 98 93 105 120 127 133 146 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28707</th>\n",
              "      <td>222 227 203 90 86 90 84 77 94 87 99 119 134 14...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28708</th>\n",
              "      <td>195 199 205 206 205 203 206 209 208 210 212 21...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>28709 rows  1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  pixels\n",
              "0      70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...\n",
              "1      151 150 147 155 148 133 111 140 170 174 182 15...\n",
              "2      231 212 156 164 174 138 161 173 182 200 106 38...\n",
              "3      24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...\n",
              "4      4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...\n",
              "...                                                  ...\n",
              "28704  84 85 85 85 85 85 85 85 86 86 86 87 86 86 91 9...\n",
              "28705  114 112 113 113 111 111 112 113 115 113 114 11...\n",
              "28706  74 81 87 89 95 100 98 93 105 120 127 133 146 1...\n",
              "28707  222 227 203 90 86 90 84 77 94 87 99 119 134 14...\n",
              "28708  195 199 205 206 205 203 206 209 208 210 212 21...\n",
              "\n",
              "[28709 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGSGqCyHP7SH",
        "outputId": "829e9858-8c1d-49dd-e691-2e4571cad094"
      },
      "source": [
        "y_train = y_train.to_numpy()\n",
        "y_train.flatten()\n",
        "y_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28709, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCKw7xjBTxHN"
      },
      "source": [
        "#eg = x_train.iloc[0]['pixels']\n",
        "\n",
        "#img = row['pixels']\n",
        "#arr = np.array(eg.split(' '))\n",
        "#arr = arr.astype(np.int)\n",
        "#arr.flatten()\n",
        "#arr = arr.reshape(48,48)\n",
        "#print(arr.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q32Vii4_RIrc",
        "outputId": "2833462f-7c71-4946-dc2a-db6a7e79a8e7"
      },
      "source": [
        "image_list = []\n",
        "for index, row in x_train.iterrows(): \n",
        "    img = row['pixels']\n",
        "    arr = np.array(img.split(' '))\n",
        "    arr = arr.astype(np.float32)\n",
        "    arr.flatten()\n",
        "    arr = arr.reshape(48,48,1)\n",
        "    image_list.append(arr)\n",
        "x_train = np.array(image_list)\n",
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28709, 48, 48, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH8pJo4yND8g"
      },
      "source": [
        "class Conv2DModel(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Convolutions\n",
        "        self.bnor1 = tf.keras.layers.BatchNormalization()\n",
        "        self.conv1 = tf.compat.v2.keras.layers.Conv2D(32, (3, 3), input_shape = (48,48,1), activation='relu', name=\"conv1\", data_format='channels_last',padding = 'same')\n",
        "        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=(2, 2), data_format='channels_last',padding = 'same')\n",
        "        self.bnor2 = tf.keras.layers.BatchNormalization()\n",
        "        self.conv2 = tf.compat.v2.keras.layers.Conv2D(32, (3, 3), activation='relu', name=\"conv2\", data_format='channels_last',padding = 'same')\n",
        "        self.pool2 = tf.keras.layers.MaxPool2D(pool_size=(2, 2), data_format='channels_last',padding = 'same')\n",
        "        self.bnor3 = tf.keras.layers.BatchNormalization()\n",
        "        self.conv3 = tf.compat.v2.keras.layers.Conv2D(64, (3, 3), activation='relu', name=\"conv3\", data_format='channels_last',padding = 'same')\n",
        "        self.pool3 = tf.keras.layers.MaxPool2D(pool_size=(2, 2), data_format='channels_last',padding = 'same')\n",
        "        self.bnor4 = tf.keras.layers.BatchNormalization()\n",
        "\n",
        "        # Flatten layer\n",
        "        self.flatten =  tf.keras.layers.Flatten(name=\"flatten\")\n",
        "        # Dense layers\n",
        "        self.d1 = tf.keras.layers.Dense(128, activation='relu', name=\"d1\")\n",
        "        self.bnor5 = tf.keras.layers.BatchNormalization()\n",
        "        self.out = tf.keras.layers.Dense(7, activation='softmax', name=\"output\")\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.bnor1(x)\n",
        "        x = self.conv1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.bnor2(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.bnor3(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.pool3(x)\n",
        "        x = self.bnor4(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.d1(x)\n",
        "        x = self.bnor5(x)\n",
        "        return self.out(x)\n",
        "\n",
        "\n",
        "# instance of the model\n",
        "model = Conv2DModel()\n",
        "\n",
        "# initializing loss function and optimizer\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics = ['accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N18r2ErcWuo4",
        "outputId": "94953fad-1aac-421f-8794-5dd8a725fffa"
      },
      "source": [
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=32,\n",
        "                    epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "898/898 [==============================] - 99s 108ms/step - loss: 1.7358 - accuracy: 0.3578\n",
            "Epoch 2/10\n",
            "898/898 [==============================] - 97s 108ms/step - loss: 1.2845 - accuracy: 0.5163\n",
            "Epoch 3/10\n",
            "898/898 [==============================] - 97s 108ms/step - loss: 1.1268 - accuracy: 0.5783\n",
            "Epoch 4/10\n",
            "898/898 [==============================] - 97s 108ms/step - loss: 0.9849 - accuracy: 0.6401\n",
            "Epoch 5/10\n",
            "898/898 [==============================] - 96s 107ms/step - loss: 0.8478 - accuracy: 0.6950\n",
            "Epoch 6/10\n",
            "898/898 [==============================] - 97s 109ms/step - loss: 0.7151 - accuracy: 0.7434\n",
            "Epoch 7/10\n",
            "898/898 [==============================] - 96s 107ms/step - loss: 0.5937 - accuracy: 0.7923\n",
            "Epoch 8/10\n",
            "898/898 [==============================] - 97s 108ms/step - loss: 0.4908 - accuracy: 0.8304\n",
            "Epoch 9/10\n",
            "898/898 [==============================] - 97s 108ms/step - loss: 0.4041 - accuracy: 0.8626\n",
            "Epoch 10/10\n",
            "898/898 [==============================] - 97s 108ms/step - loss: 0.3255 - accuracy: 0.8900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgUNpmxcZABb"
      },
      "source": [
        "model.save_weights('weights/w.tf', save_format='tf')\n",
        "#model.save_weights(\"model1.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaQ0lpM0eDNX",
        "outputId": "a4e075a3-5d1d-436b-fce3-b2ff0af6749b"
      },
      "source": [
        "image_list = []\n",
        "for index, row in x_test.iterrows(): \n",
        "    img = row['pixels']\n",
        "    arr = np.array(img.split(' '))\n",
        "    arr = arr.astype(np.float32)\n",
        "    arr.flatten()\n",
        "    arr = arr.reshape(48,48,1)\n",
        "    image_list.append(arr)\n",
        "x_test = np.array(image_list)\n",
        "x_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7178, 48, 48, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xv8BuvkBc0kB"
      },
      "source": [
        "y_pred = model.predict(x_test)\n",
        "y_pred = np.argmax(y_pred, axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5x4b6PteXkJ",
        "outputId": "759b05bd-b7eb-4f1b-e253-599937512ced"
      },
      "source": [
        "y_test = y_test.to_numpy()\n",
        "y_test.flatten()\n",
        "y_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7178, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVbU_LFLc4KY",
        "outputId": "08b715fd-6e85-4670-f562-e784849b18ad"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "\n",
        "\n",
        "print(precision_score(y_test, y_pred, average='macro'))\n",
        "\n",
        "\n",
        "print(recall_score(y_test, y_pred, average='macro'))\n",
        "\n",
        "\n",
        "print(f1_score(y_test, y_pred, average='macro'))\n",
        "\n",
        "# 0.556143772638618\n",
        "# 0.548739816446838\n",
        "# 0.534934668633245\n",
        "# 0.540823650376931"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.556143772638618\n",
            "0.548739816446838\n",
            "0.534934668633245\n",
            "0.540823650376931\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQGBR4rXfDvR"
      },
      "source": [
        "# using Gaussian Naive Bayes - just a trial - didn't work\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "x_train.flatten()\n",
        "x_train2 = x_train.reshape(28709,2304)\n",
        "x_test.flatten()\n",
        "x_test2 = x_test.reshape(7178,2304)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tab3RpUTgXBZ",
        "outputId": "29ecbe7e-41a4-4118-d091-3d3373d312fa"
      },
      "source": [
        " #initializing the GaussianNB model\n",
        "model = GaussianNB()\n",
        "\n",
        "    #fitting the model using the training samples\n",
        "model.fit(x_train2,y_train)\n",
        "\n",
        "    #predicting the target values for test data\n",
        "y_pred = model.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/naive_bayes.py:206: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianNB(priors=None, var_smoothing=1e-09)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNYRnpJtgvfW",
        "outputId": "d9a785de-028d-4ab0-9e5d-aad347350917"
      },
      "source": [
        "y_pred = model.predict(x_test2)\n",
        "accuracy_score(y_test,y_pred)\n",
        "\n",
        "# accuracy = 0.22652549456673168"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.22652549456673168"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    }
  ]
}